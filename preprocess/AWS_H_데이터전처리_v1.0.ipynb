{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *방재기상관측(AWS) 데이터 전처리 시간별*\n",
    "#### - 250개 시군구 기준으로 날씨를 정리함\n",
    "> 1. 시군구의 위경도와 센서간의 유클리드 거리 계산\n",
    "    - 시군구 위경도 기준으로 센서들의 위경도로 유클리드 거리를 추출\n",
    "> 2. 거리 별 가중치를 계산하여 각 날씨 관련 변수를 계산\n",
    "    - 정리된 시군구와 센서의 거리 정보를 실제 센서 값을 결합함 (온도가 없는 경우를 제거하여 \n",
    "      실데이터가 존재하는 센서만 사용)\n",
    "    - 최고, 최저 온도 발생 시각 및 최고풍속 발생 시각은 초 단위로 계산하여 가중치 적용\n",
    "    - 결합된 센서들 중 top3에 가중치를 적용하여 가 지역별 변수값 추출\n",
    "    \n",
    "#### - 가중치 적용 공식\n",
    ">  - w1 = 1/2 * (1 - {|d-a1|/(|d-a1| + |d-a2| + |d-a3| + |d-a4|)})\n",
    ">  - w2 = 1/2 * (1 - {|d-a2|/(|d-a1| + |d-a2| + |d-a3| + |d-a4|)})\n",
    ">  - w3 = 1/2 * (1 - {|d-a3|/(|d-a1| + |d-a2| + |d-a3| + |d-a4|)})\n",
    "\n",
    "### - 원천 데이터 리스트\n",
    ">  - sigungu_xy.csv (시군구)\n",
    ">  - censerinfo.csv (센서데이터)\n",
    ">  - OBS_AWS_HH_202001_202006.csv (센서별 날씨관련 정보)\n",
    ">  - OBS_AWS_HH_202007_202010.csv (센서별 날씨관련 정보)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>wtw</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20bd9217708>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean,col,split, col, regexp_extract, when, lit\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[6]\") \\\n",
    "                    .appName('wtw') \\\n",
    "                    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 250개의 시군구 위경도 로딩하여 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 시군구 위경도 로딩 컬럼 타입 수정 및 sql사용을 위해 데이터셋 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "SigunguXY = spark\\\n",
    "          .read\\\n",
    "          .option(\"inferSchema\", \"true\")\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .csv(\"data/sigungu_xy.csv\")\n",
    "SigunguXY = SigunguXY.withColumn(\"sigungu_x\", SigunguXY.sigungu_x.cast(\"float\")).withColumn(\"sigungu_y\", SigunguXY.sigungu_y.cast(\"float\"))\n",
    "SigunguXY.createOrReplaceTempView(\"Sigunguxy\")\n",
    "\n",
    "print(SigunguXY.count())\n",
    "# 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "250\n",
      "+-------+--------+-------+------------------+------------------+\n",
      "|summary|    sido|sigungu|         sigungu_x|         sigungu_y|\n",
      "+-------+--------+-------+------------------+------------------+\n",
      "|  count|     250|    250|               250|               250|\n",
      "|   mean|    null|   null|127.63907656860351| 36.41521324157715|\n",
      "| stddev|    null|   null| 0.912011813226167|1.0729488769413968|\n",
      "|    min|  강원도| 가평군|         124.67009|         33.254066|\n",
      "|    max|충청북도| 횡성군|         130.90572|         38.380592|\n",
      "+-------+--------+-------+------------------+------------------+\n",
      "\n",
      "+------+-------+---------+---------+\n",
      "|  sido|sigungu|sigungu_x|sigungu_y|\n",
      "+------+-------+---------+---------+\n",
      "|강원도| 강릉시| 128.8759| 37.75211|\n",
      "|강원도| 고성군|128.46786|38.380592|\n",
      "+------+-------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(SigunguXY.count())\n",
    "print(SigunguXY.distinct().count())\n",
    "SigunguXY.describe().show()\n",
    "SigunguXY.show(2)\n",
    "# 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 524개의 센서 데이터를 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 데이터 로딩, 컬럼 타입 수정 및 sql사용을 위해 데이터셋 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1893\n"
     ]
    }
   ],
   "source": [
    "SenserInfo = spark\\\n",
    "          .read\\\n",
    "          .option(\"inferSchema\", \"true\")\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .csv(\"data/censerinfo.csv\")\n",
    "SenserInfo = SenserInfo.withColumn(\"x\", SenserInfo.x.cast(\"float\")).withColumn(\"y\", SenserInfo.y.cast(\"float\"))\n",
    "SenserInfo.createOrReplaceTempView(\"SenserInfo\")\n",
    "\n",
    "print(SenserInfo.count())\n",
    "# 1893"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 데이터 정리 - 이력관리 되고 있는 센서 위치 정보를 최근으로 데이터로 가져 오고 필요 데이터만 으로 subset을 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1893\n",
      "+-------+-----------------+-------------------+-------------------+-----------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|         senserid|          startdate|            enddate|sernsername|                 x|                 y|region_1depth_name|region_2depth_name|region_3depth_name|\n",
      "+-------+-----------------+-------------------+-------------------+-----------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|             1893|               1893|               1893|       1893|              1893|              1893|              1893|              1893|              1893|\n",
      "|   mean|693.0612783940835|               null|               null|       null|127.53129761419281|36.158902869045704|              null|              null|              null|\n",
      "| stddev| 174.599815603351|               null|               null|       null|0.9911930175069503|1.3313135706812764|              null|              null|              null|\n",
      "|    min|               12|1989-06-27 00:00:00|1996-12-31 00:00:00|     가거도|          124.6305|           33.1137|            강원도|            가평군|            가산동|\n",
      "|    max|              978|2020-10-30 00:00:00|2222-12-31 00:00:00|       후포|          131.8698|           38.5441|          충청북도|            횡성군|            흑산면|\n",
      "+-------+-----------------+-------------------+-------------------+-----------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "print(SenserInfo.distinct().count())\n",
    "\n",
    "SenserInfo.describe().show()\n",
    "# SenserInfo.show()\n",
    "# 1893"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 유지되고 있는 센서 이력은 enddate가 공백이고 유지되지 않는 센서는 enddate에 작동 중지 일이 등록 되어 있음 (csv파일에 enddate가 공백인 경우 큰 수 2222년을 등록)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------+------------------+------------------+---------------+---------------+---------------+\n",
      "|summary|         senserid|sernsername|          senser_x|          senser_y|senser_region_1|senser_region_2|senser_region_3|\n",
      "+-------+-----------------+-----------+------------------+------------------+---------------+---------------+---------------+\n",
      "|  count|              524|        524|               524|               524|            524|            524|            524|\n",
      "|   mean|679.4561068702291|       null|127.52589231593008|36.187192130634806|           null|           null|           null|\n",
      "| stddev|184.2299668080408|       null|1.0150116146748691| 1.364833816970453|           null|           null|           null|\n",
      "|    min|               12|     가거도|          124.6305|           33.1221|         강원도|         가산면|         가평군|\n",
      "|    max|              978|       횡성|          131.8698|           38.5439|       충청북도|         흑산면|         횡성군|\n",
      "+-------+-----------------+-----------+------------------+------------------+---------------+---------------+---------------+\n",
      "\n",
      "524\n"
     ]
    }
   ],
   "source": [
    "LastSenserInfo = spark.sql(\"\"\"\n",
    "SELECT A.senserid, A.sernsername, A.x AS senser_x, A.y AS senser_y,\n",
    "        A.region_1depth_name AS senser_region_1,  \n",
    "        A.region_3depth_name AS senser_region_2,  \n",
    "        A.region_2depth_name AS senser_region_3\n",
    "FROM SenserInfo AS A\n",
    "\n",
    "INNER JOIN (\n",
    "    SELECT senserid, max(enddate) AS lenddate\n",
    "    FROM SenserInfo\n",
    "    GROUP BY senserid\n",
    "    ) AS B\n",
    "ON A.senserid = B.senserid\n",
    "AND A.enddate = B.lenddate\n",
    "\n",
    "WHERE 1 = 1\n",
    "\n",
    "\"\"\")\n",
    "LastSenserInfo.createOrReplaceTempView(\"LastSenserInfo\")\n",
    "LastSenserInfo.describe().show()\n",
    "print(LastSenserInfo.count())\n",
    "# 524"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 시군구 와 센서의 위경도 값을 이용하여 유클리드 거리 계산 (250 * 524 = 131000의 조합)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 시군구별 전체 센서의 조합을 위해 full join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131000\n",
      "+------+-------+---------+---------+--------+-----------+--------+--------+---------------+---------------+---------------+\n",
      "|  sido|sigungu|sigungu_x|sigungu_y|senserid|sernsername|senser_x|senser_y|senser_region_1|senser_region_2|senser_region_3|\n",
      "+------+-------+---------+---------+--------+-----------+--------+--------+---------------+---------------+---------------+\n",
      "|강원도| 강릉시| 128.8759| 37.75211|      12| 안면도(감)|126.3167| 36.5333|       충청남도|         안면읍|         태안군|\n",
      "|강원도| 고성군|128.46786|38.380592|      12| 안면도(감)|126.3167| 36.5333|       충청남도|         안면읍|         태안군|\n",
      "+------+-------+---------+---------+--------+-----------+--------+--------+---------------+---------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sigungu_dist = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM SigunguXY AS A\n",
    "\n",
    "FULL JOIN LastSenserInfo AS B\n",
    "\n",
    "WHERE 1 = 1\n",
    "-- AND A.sigungu = '강릉시'\n",
    "\n",
    "\"\"\")\n",
    "Sigungu_dist.createOrReplaceTempView(\"Sigungu_dist\")\n",
    "print(Sigungu_dist.count())\n",
    "Sigungu_dist.show(2)\n",
    "# 250 * 524 = 131000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 유클리드 거리 및 순위 추출 (pyspark dataframe에서 처리 어려워 pandas 객체로 변환하여처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8346126716273883\n"
     ]
    }
   ],
   "source": [
    "# 유클리드 거리 계산 함수\n",
    "from scipy.spatial import distance\n",
    "print(distance.euclidean((128.8759061, 37.75210808), (126.3167, 36.5333)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환 전 <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "변환 후 <class 'pandas.core.frame.DataFrame'>\n",
      "유클리드 거리 추가 후 다시변환 <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "131000\n",
      "+------+-------+------------------+------------------+--------+-----------+------------------+-----------------+---------------+---------------+---------------+------------------+\n",
      "|  sido|sigungu|         sigungu_x|         sigungu_y|senserid|sernsername|          senser_x|         senser_y|senser_region_1|senser_region_2|senser_region_3|         udistance|\n",
      "+------+-------+------------------+------------------+--------+-----------+------------------+-----------------+---------------+---------------+---------------+------------------+\n",
      "|강원도| 강릉시| 128.8759002685547| 37.75210952758789|      12| 안면도(감)|126.31670379638672|36.53329849243164|       충청남도|         안면읍|         태안군| 2.834605249867365|\n",
      "|강원도| 고성군|128.46786499023438|38.380592346191406|      12| 안면도(감)|126.31670379638672|36.53329849243164|       충청남도|         안면읍|         태안군|2.8354874473456375|\n",
      "+------+-------+------------------+------------------+--------+-----------+------------------+-----------------+---------------+---------------+---------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark dataframe에서 처리 어려워 pandas 객체로 변환하여처리\n",
    "from scipy.spatial import distance\n",
    "# print(distance.euclidean((128.8759061, 37.75210808), (126.3167, 36.5333)))\n",
    "print('변환 전', type(Sigungu_dist))\n",
    "# Sigunguxy_dist['distance'] = Sigunguxy_dist.map(lambda x : distance.euclidean((x['sigungu_x'], x['sigungu_y']), (x['x'], x['y'])), axis=1)\n",
    "\n",
    "#유클리드 함수가 안 돌아 가서 pdf로 변환 후 처리\n",
    "pdf_Sigungu_dist = Sigungu_dist.toPandas()\n",
    "print('변환 후', type(pdf_Sigungu_dist))\n",
    "pdf_Sigungu_dist['udistance'] = pdf_Sigungu_dist.apply(lambda x : distance.euclidean((x['sigungu_x'], x['sigungu_y']), (x['senser_x'], x['senser_y'])), axis=1)\n",
    "# 다시 스파크 프레임으로 변환\n",
    "Sigungu_dist = spark.createDataFrame(pdf_Sigungu_dist)\n",
    "Sigungu_dist.createOrReplaceTempView(\"Sigungu_dist\")\n",
    "print('유클리드 거리 추가 후 다시변환', type(Sigungu_dist))\n",
    "print(Sigungu_dist.count())\n",
    "Sigungu_dist.show(2)\n",
    "# 131000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">  *여기서 시도군 별 top3를 추출하여 모수를 줄여서 진행하면 좋은데 실제 센서 데이터 중 top3에 빠지는 경우가 발생하여 전체를 모든 센서와 비교하여 top3를 추출해야 함 - 로직수정 ㅠㅠ*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 일/시간별 센서 정보에 정리된 시군구 * 센서데이터 mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 날씨정보 로딩 및 필요 정보 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2216231\n"
     ]
    }
   ],
   "source": [
    "AWS = spark.read.format(\"csv\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"charset\",\"euc-kr\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .load(\"data/OBS_AWS_HH_201907*.csv\")\n",
    "#             .load(\"data/OBS_AWS_HH_20*.csv\")\n",
    "\n",
    "AWS = AWS.toDF(\"senserid\", \"sersername\", \"datetime\", \"temp\", \"winddeg\", \"windspeed\", \\\n",
    "               \"rainfall\", \"placehpa\", \"seahpa\", \"humidity\")\n",
    "AWS.createOrReplaceTempView(\"AWS\")\n",
    "\n",
    "print(AWS.count())\n",
    "# AWS.show(2)\n",
    "# AWS.printSchema()\n",
    "# 702506\n",
    "# 16826600 전체 - 내컴으론 불가능\n",
    "# 4381480 2017\n",
    "# 4354207 2018\n",
    "# 2174118 20191H\n",
    "# 2216231 20192H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553321000\n"
     ]
    }
   ],
   "source": [
    "SigunguWeather = spark.sql(\"\"\"\n",
    "SELECT A.sido, A.sigungu, B.senserid, B.sersername, A.udistance, B.datetime\n",
    "       , B.temp\n",
    "       , B.winddeg\n",
    "       , B.windspeed\n",
    "       , B.rainfall\n",
    "       , B.placehpa\n",
    "       , B.seahpa\n",
    "       , B.humidity\n",
    "FROM Sigungu_dist AS A\n",
    "\n",
    "INNER JOIN AWS AS B\n",
    "ON A.senserid = B.senserid\n",
    "AND B.temp is not null\n",
    "-- AND B.rainfall is not null \n",
    "-- AND B.humidity is not null\n",
    "-- 센서 이상으로 온도가 나오지 않는 센서는 제거 하고 key별로 가까운 센서를 추출\n",
    "WHERE 1 = 1\n",
    "\n",
    "\"\"\")\n",
    "SigunguWeather.createOrReplaceTempView(\"SigunguWeather\")\n",
    "\n",
    "print(SigunguWeather.count())\n",
    "# SigunguWeather.show(2)\n",
    "# 일\n",
    "# 175545250 # 하나의 센서가 여러 sigungu에서 쓰일 수 있음 ㅋㅋㅋㅋ 일 4년치 2억건 \n",
    "# 온도의 null 제거 후 174405250\n",
    "# 시간\n",
    "# 4204704250 # 하나의 센서가 여러 sigungu에서 쓰일 수 있음 ㅋㅋㅋㅋ 일 4년치 41억건 인생 최고\n",
    "# 온도의 null 제거 후 4199398000 41억\n",
    "# 1094328250 2017 10억\n",
    "# 1086771750 2018\n",
    "# 553321000 2017 H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 유클리드 거리에 따라 작은 순으로 key별로 순위를 주고 top3 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553321000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# 시군구 기준으로 센서들의 udistance에 따라 순위를 줌 \n",
    "windowSpec  = Window.partitionBy(\"sido\", \"sigungu\", \"datetime\").orderBy(\"udistance\")\n",
    "SigunguWeather = SigunguWeather.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "\n",
    "SigunguWeather.createOrReplaceTempView(\"SigunguWeather\")\n",
    "\n",
    "print(SigunguWeather.count())\n",
    "# SigunguWeather.show(2)\n",
    "# 174405250\n",
    "# 4199398000\n",
    "# 1094328250 2017\n",
    "# 1086771750 2018\n",
    "# 553321000 2019 H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3312000\n"
     ]
    }
   ],
   "source": [
    "# udistance에 따른 순위 중 상위 3개만 추출\n",
    "SigunguSenserTop3 = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM SigunguWeather \n",
    "    WHERE 1 = 1\n",
    "    AND row_number IN (1, 2, 3)\n",
    "    \"\"\")\n",
    "SigunguSenserTop3.createOrReplaceTempView(\"SigunguSenserTop3\")\n",
    "\n",
    "print(SigunguSenserTop3.count())\n",
    "# SigunguSenserTop3.show(2)\n",
    "# 2017\t365 * 24 = 8760\n",
    "# 2018\t365 * 24 = 8760\n",
    "# 2019\t365 * 24 = 8760\n",
    "# 2020\t305 * 24 = 7320\n",
    "# 합게 33600 시간\n",
    "# 25200000 = 250sigungu * 3senser * 33600hours = 2천5백만\n",
    "# 6570000 = 250sigungu * 3senser * 8760hours = 6백50만건 2017\n",
    "# 6570000 2018\n",
    "# 3312000 2019 H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 좀 걸림 필요 없음\n",
    "# maxSql = spark.sql(\"\"\"\n",
    "#     SELECT sido\n",
    "#         , sigungu\n",
    "#         , datetime\n",
    "#         , count(*)\n",
    "#     FROM SigunguSenserTop3\n",
    "#     WHERE 1 = 1\n",
    "#     GROUP BY sido, sigungu, datetime\n",
    "#     HAVING count(*) < 3\n",
    "# \"\"\")\n",
    "# maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function use to print feature with null values and null count \n",
    "# def null_value_count(df):\n",
    "#   null_columns_counts = []\n",
    "#   numRows = df.count()\n",
    "#   for k in df.columns:\n",
    "#     nullRows = df.where(col(k).isNull()).count()\n",
    "#     if(nullRows > 0):\n",
    "#       temp = k,nullRows\n",
    "#       null_columns_counts.append(temp)\n",
    "#   return(null_columns_counts)\n",
    "\n",
    "# 겁나 오래 돌아 감 결과를 못 봤음 ㅋ\n",
    "# null_columns_count_list = null_value_count(SigunguSenserTop3)\n",
    "# null_columns_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 겁나 오래 돌아 감 ㅋ\n",
    "# LastSenserInfo.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 센서와 시군구 거리에 의한 가중치 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3312000\n"
     ]
    }
   ],
   "source": [
    "# w1 = 1/2 * [1 - {|d-a1|/(|d-a1| + |d-a2| + |d-a3| + |d-a4|)}]\n",
    "# w2 = 1/2 * [1 - {|d-a2|/(|d-a1| + |d-a2| + |d-a3| + |d-a4|)}]\n",
    "# w3 = 1/2 * [1 - {|d-a3|/(|d-a1| + |d-a2| + |d-a3| + |d-a4|)}]\n",
    "from pyspark.sql.functions import row_number, sum\n",
    "\n",
    "windowSp  = Window.partitionBy(\"sido\", \"sigungu\", \"datetime\")\n",
    "SigunguSenserTop3 = SigunguSenserTop3.withColumn(\"sumUdist\", sum(SigunguSenserTop3.udistance).over(windowSp))\n",
    "\n",
    "SigunguSenserTop3 = SigunguSenserTop3.withColumn(\"weight\", (1/2 * (1 - SigunguSenserTop3.udistance / SigunguSenserTop3.sumUdist)))\n",
    "SigunguSenserTop3.createOrReplaceTempView(\"SigunguSenserTop3\")\n",
    "\n",
    "print(SigunguSenserTop3.count())\n",
    "# SigunguSenserTop3.show(2)\n",
    "# 6570000 2017\n",
    "# 6570000 2018\n",
    "# 3312000 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증\n",
    "# maxSql = spark.sql(\"\"\"\n",
    "#     SELECT *\n",
    "#     FROM SigunguSenserTop3\n",
    "#     WHERE 1 = 1\n",
    "#     AND sido = '강원도'\n",
    "#     AND sigungu = '강릉시'\n",
    "#     AND datetime = '2019-09-01 00:00'\n",
    "# \"\"\")\n",
    "\n",
    "# maxSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 가중치를 적용한 일별 시군구의 날씨 정보 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3312000\n"
     ]
    }
   ],
   "source": [
    "# 각 변수에 가중치를 적용함 1\n",
    "SigunguSenserTop3_w = spark.sql(\"\"\"\n",
    "    SELECT   *\n",
    "            ,temp           * weight  AS w_temp      \n",
    "            ,winddeg        * weight  AS w_winddeg   \n",
    "            ,windspeed      * weight  AS w_windspeed \n",
    "            ,rainfall       * weight  AS w_rainfall  \n",
    "            ,placehpa       * weight  AS w_placehpa  \n",
    "            ,seahpa         * weight  AS w_seahpa    \n",
    "            ,humidity       * weight  AS w_humidity  \n",
    "    FROM SigunguSenserTop3 \n",
    "    WHERE 1 = 1\n",
    "    \"\"\")\n",
    "SigunguSenserTop3_w.createOrReplaceTempView(\"SigunguSenserTop3_w\")\n",
    "\n",
    "print(SigunguSenserTop3_w.count())\n",
    "# SigunguSenserTop3_w.show(2)\n",
    "# 1050000 daily\n",
    "# 6570000 2017\n",
    "# 6570000 2018\n",
    "# 3312000 2019 H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104000\n"
     ]
    }
   ],
   "source": [
    "# 각 변수에 가중치를 적용함 2\n",
    "AWS_D_Weather = spark.sql(\"\"\"\n",
    "    SELECT  datetime \n",
    "            ,sido\n",
    "            ,sigungu\n",
    "            \n",
    "            ,sum(w_temp)                 AS w_temp      \n",
    "            ,sum(w_winddeg)              AS w_winddeg   \n",
    "            ,sum(w_windspeed)            AS w_windspeed \n",
    "            ,sum(w_rainfall)             AS w_rainfall  \n",
    "            ,sum(w_placehpa)             AS w_placehpa\n",
    "            ,sum(w_seahpa)               AS w_seahpa    \n",
    "            ,sum(w_humidity)             AS w_humidity  \n",
    "            \n",
    "    FROM SigunguSenserTop3_w \n",
    "    WHERE 1 = 1\n",
    "    GROUP BY datetime  \n",
    "            ,sido\n",
    "            ,sigungu\n",
    "    \"\"\")\n",
    "AWS_D_Weather.createOrReplaceTempView(\"AWS_D_Weather\")\n",
    "\n",
    "print(AWS_D_Weather.count())\n",
    "# AWS_D_Weather.show(2)\n",
    "# 250 * 1400 = 350000\n",
    "# 250 * 365 * 24 = 2190000 2017\n",
    "# 2190000 2018\n",
    "# 1104000 2019 H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o139.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 105 in stage 82.0 failed 1 times, most recent failure: Lost task 105.0 in stage 82.0 (TID 3658, localhost, executor driver): java.io.IOException: 디스크 공간이 부족합니다\r\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\r\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:75)\r\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\r\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\r\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\r\n\tat sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)\r\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)\r\n\tat org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:429)\r\n\tat org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedDataWithChannel(BypassMergeSortShuffleWriter.java:232)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:205)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:167)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: 디스크 공간이 부족합니다\r\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\r\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:75)\r\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\r\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\r\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\r\n\tat sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)\r\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)\r\n\tat org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:429)\r\n\tat org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedDataWithChannel(BypassMergeSortShuffleWriter.java:232)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:205)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:167)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9b33d814f7a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 판다스 데이터 프레임으로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpdf_AWS_D_Weather\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAWS_D_Weather\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# csv로 저장 na 포함\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpdf_AWS_D_Weather\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./AWS_H_Weather_na_2019_2H.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pydatavenv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pydatavenv\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    594\u001b[0m         \"\"\"\n\u001b[0;32m    595\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pydatavenv\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pydatavenv\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pydatavenv\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o139.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 105 in stage 82.0 failed 1 times, most recent failure: Lost task 105.0 in stage 82.0 (TID 3658, localhost, executor driver): java.io.IOException: 디스크 공간이 부족합니다\r\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\r\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:75)\r\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\r\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\r\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\r\n\tat sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)\r\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)\r\n\tat org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:429)\r\n\tat org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedDataWithChannel(BypassMergeSortShuffleWriter.java:232)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:205)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:167)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: 디스크 공간이 부족합니다\r\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\r\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:75)\r\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\r\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\r\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\r\n\tat sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)\r\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)\r\n\tat org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:429)\r\n\tat org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedDataWithChannel(BypassMergeSortShuffleWriter.java:232)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:205)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:167)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# 판다스 데이터 프레임으로 변환\n",
    "pdf_AWS_D_Weather = AWS_D_Weather.toPandas()\n",
    "\n",
    "# csv로 저장 na 포함\n",
    "pdf_AWS_D_Weather.to_csv(\"./AWS_H_Weather_na_2019_2H.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최고최저 온도 최고 시간은 이전 데이터로 null 처리\n",
    "display(pdf_AWS_D_Weather.isnull().sum())\n",
    "pdf_AWS_D_Weather.sort_values(by=['sido', 'sigungu', 'datetime'], axis=0, inplace=True)\n",
    "\n",
    "pdf_AWS_D_Weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_AWS_D_Weather.fillna(method='ffill', inplace=True)\n",
    "display(pdf_AWS_D_Weather.isnull().sum())\n",
    "\n",
    "pdf_AWS_D_Weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv로 저장\n",
    "pdf_AWS_D_Weather.to_csv(\"./AWS_H_Weather_2019_2H.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seoulFlag = pdf_AWS_D_Weather['sido'] == '서울특별시'\n",
    "datetimeFlag = pdf_AWS_D_Weather['datetime'] >= '2019-12-01 00:00'\n",
    "pdf_AWS_D_Weather_test = pdf_AWS_D_Weather[seoulFlag & datetimeFlag]\n",
    "\n",
    "display(pdf_AWS_D_Weather_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib 한글 폰트 오류 문제 해결\n",
    "from matplotlib import font_manager, rc\n",
    "font_path = \"C:/PyStexam/data/THEdog.ttf\"   #폰트파일의 위치\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "plt.figure(figsize=(35, 15))\n",
    "sns.lineplot(x=\"datetime\", y=\"w_temp\", hue=\"sigungu\", data=pdf_AWS_D_Weather_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(35, 15))\n",
    "sns.boxplot(x='datetime', y='w_temp', hue='sigungu', data=pdf_AWS_D_Weather_test) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydatavenv",
   "language": "python",
   "name": "pydatavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
